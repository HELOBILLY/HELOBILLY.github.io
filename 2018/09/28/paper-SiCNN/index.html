






<!doctype html>
<html lang="zh-CN">
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="author" content="HELOBILLY">
  
  
  
  
    <meta name="description" content="An object-based supervised classification framework for very-high-resolution remote sensing images using convolutional neural networks 链接Zhang X, Wang Q, Chen G, et al.Remote Sensing Letters. 2018,...">
  
  <title>一种基于卷积神经网络（CNN）的高分辨率遥感影像分类方法 [ HELOBILLY ]</title>
  
  
    <link rel="shortcut icon" href="/logo.jpg">
  
  
  <link rel="stylesheet" href="/css/random.css">
<link rel="stylesheet" href="/css/vegas.min.css">
<link rel="stylesheet" href="/css/highlight-railscasts.css">
<link rel="stylesheet" href="/css/jquery.fancybox.css">
<link rel="stylesheet" href="/css/iconfont/iconfont.css">
<link rel="stylesheet" href="/css/jquery.fancybox-thumbs.css">
<link rel="stylesheet" href="/css/plyr.css">
  
</head>

<body>
<div class="side-navigate hide-area">
  
    <div class="item prev">
      <a href="/2018/09/27/paper-sdfcn/">
        <div class="item-icon"></div>
      </a>
      <div class="item-title">
        基于深度对称密集连接全卷积网络的高分辨率遥感影像语义分割框架 SDFCN
      </div>
    </div>
  
  
    <div class="item next">
      <a href="/2018/09/29/paper-kdcnn/">
        <div class="item-icon"></div>
      </a>
      <div class="item-title">
        基于知识蒸馏的小规模遥感影像场景分类网络模型训练方法
      </div>
    </div>
  
</div>
<div id="outer-container" class="hide-area">
<div id="container">
  <div id="menu-outer" class="slide-down">
    <div id="menu-inner">
      <div id="brand">
        
        <a onClick="openUserCard()">
          <img id="avatar" src="/helobilly.jpg"/>
          <div id="homelink">HELOBILLY</div>
        </a>
      </div>
      <div id="menu-list">
        <ul>
        
        
          
            <li>
          
            <a href="/index.html">Home</a>
            
          </li>
        
          
            <li>
          
            <a href="/archives">Archives</a>
            
          </li>
        
          
            <li>
          
            <a href="/works">Works</a>
            
          </li>
        
          
            <li>
          
            <a href="/about">About</a>
            
          </li>
        
        </ul>
      </div>
      <div id="show-menu">
        <button>Menu</button>
      </div>
    </div>
  </div>

  <div id="content-outer">
    <div id="content-inner">
      
      
  <article id="post">
    <h1>一种基于卷积神经网络（CNN）的高分辨率遥感影像分类方法</h1>
    <p class="page-title-sub">
      <span id = "post-title-date">撰写于 2018-09-28</span>
      
        <span id = "post-title-updated">修改于 2019-04-16</span>
      
      
      <span id = "post-title-categories">分类
      
      
        
        
        <a href="/categories/遥感影像分类/">遥感影像分类</a>
      
      </span>
      
      
      <span id = "post-title-tags">
      标签
      
      
        
        
        <a href="/tags/深度学习/">深度学习</a>
      
        
          /
        
        
        <a href="/tags/CNN/">CNN</a>
      
        
          /
        
        
        <a href="/tags/遥感影像分类/">遥感影像分类</a>
      
      </span>
      
    </p>
    
    <blockquote>
<p><strong>An object-based supervised classification framework for very-high-resolution remote sensing images using convolutional neural networks</strong> <a href="http://www.tandfonline.com/doi/full/10.1080/2150704X.2017.1422873" target="_blank" rel="external">链接</a><br>Zhang X, Wang Q, Chen G, et al.<br>Remote Sensing Letters. 2018, 9(4): 373-382.<code>SCI</code></p>
</blockquote>
<p>基于对象的影像分类方法(<strong>Object-based image classification，OBIC</strong>)是针对高分辨率(<strong>very-high-resolution，VHR</strong>)遥感影像分类方法中基于像素的图像分类(<strong>pixel-based image classification，PBIC</strong>)的不足而提出的，然而，OBIC中的大多数分类方法都是处理从分割影像对象中提取的一维特征。为了提取分割对象的二维深度特征，本文采用卷积神经网络(CNNs)，提出了一种新的深度OBIC框架。首先分析了分割对象的不同掩码策略，并设计了两种网络架构。对比实验结果表明，<strong>DiCNN-4(Double-input CNN)</strong>在实验数据集上获得了比传统OBIC方法更高的整体精度(OA)、Kappa系数和F1测度。</p>
<a id="more"></a>
<!-- toc -->
<h1 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h1><p>基于对象的图像分类(OBIC)已经成为超高分辨率遥感影像土地利用的主流框架。OBIC可以利用高分辨率遥感影像中的纹理、空间几何信息，与基于像素的图像分类(PBIC)框架相比，在一定程度上克服了“椒盐”现象。常见的OBIC框架包括以下步骤：</p>
<blockquote>
<ul>
<li>图像预处理</li>
<li>图像分割</li>
<li>特征提取</li>
<li>监督分类</li>
</ul>
</blockquote>
<p>OBIC中的大多数分类方法都是处理从分割对象中提取的手工制作的特征，包括光谱特征，空间特征，纹理特征和其他经典光谱植被指数(<strong>vegetation indices， VI</strong>)，如归一化差异植被指数(<strong>Normalized Difference Vegetation Index， NDVI</strong>)和归一化差异水指数(<strong>Normalized Difference Water Index， NDWI</strong>)。只有少数研究专注于自动特征提取方法，如稀疏编码和堆叠去噪自动编码器。然而，这些无监督的一维特征是从二维数据中提取的，这导致了对象信息的丢失和泛化的难度。</p>
<p>为了提取对象的二维深度特征，本文将有监督的卷积神经网络(CNN)引入到OBIC框架中。文献<sup><a href="#footnote1">1</a></sup>还提出了一种混合OBIC框架，该方法结合了手工和深度CNN提取的特征。然而，该方法并不直接处理分割对象。与这个想法不同，我们将整个分割对象输入到CNN网络中，并且仅利用CNN提取的特征。本文的贡献如下：</p>
<blockquote>
<ul>
<li>首次将CNNs引入到OBIC框架中，直接从分割对象的原始图像块中提取二维深度特征。</li>
<li>提出了两种裁剪策略来对框架中的分割对象进行预处理。</li>
<li>设计了两种CNN模型架构，同时处理分割对象的图像数据和掩码数据</li>
</ul>
</blockquote>
<h1 id="2、方法"><a href="#2、方法" class="headerlink" title="2、方法"></a>2、方法</h1><p>在本节中，我们首先探索处理分割对象掩膜(mask)的方法，并将其转换为适合CNNs的输入。同时，我们设计了CNN的两种架构来提取深度二维特征进行分类。</p>
<h2 id="2-1-分割对象预处理"><a href="#2-1-分割对象预处理" class="headerlink" title="2.1 分割对象预处理"></a>2.1 分割对象预处理</h2><p>分割后的输入图像被裁剪成不同形状和尺度的分割对象。因此，需要对分割对象的边界(形状)和原始栅格数据进行预处理，以满足CNNs的要求。这意味着输入图像的大小必须是固定的。在这种情况下，我们提出以下两种裁剪策略:</p>
<blockquote>
<ul>
<li>用最小外接矩形裁剪分割对象，将原始像素值保留在对象的边界内，并在边界外用零(黑色)填充像素(图1(b))</li>
<li>创建两种类型的数据源。第一个是原始裁剪的分割对象patch(图1(c))。另一个是分割对象的二进制掩码，边界内为零填充，外部为一填充(图1(d))</li>
</ul>
</blockquote>
<p><img src="/2018/09/28/paper-SiCNN/mask.png" alt="图1： (a)原始分割图像; (b)带掩码的裁剪对象; (c)无掩码的裁剪对象; (d)裁剪对象的二值掩膜"></p>
<h2 id="2-2-影像分类网络结构"><a href="#2-2-影像分类网络结构" class="headerlink" title="2.2 影像分类网络结构"></a>2.2 影像分类网络结构</h2><p>根据2.1中提到的两种裁剪方法，我们为OBIC设计了两种CNN架构:单输入CNN(single-input CNN， SiCNN)和双输入CNN(double-input CNN， DiCNN)。SiCNN只包含一个输入数据源，而DiCNN包含两个。它们都由几个基本的CNN层组成:卷积层、最大池化层、全连接层、修正线性单元(ReLU)激活层、全局平均池化(GAP)层和softmax输出层。它们之间的区别在于不同的输入数据源和网络分支架构，具体结构如下图：</p>
<p><img src="/2018/09/28/paper-SiCNN/structure.png" alt="图2： SiCNN和DiCNN网络结构"></p>
<blockquote>
<ul>
<li>图2(a): SiCNN-3，以裁剪对象的三通道(RGB)patch作为输入(图1(b))</li>
<li>图2(b): SiCNN-4，输入是每个裁剪对象的无掩码图像patch和二进制mask的组合，总共包含四个通道(RGB图像+二进制mask)</li>
<li>图2(c): DiCNN-a包含两个完整的CNNs，分别从图像和掩码中学习特征并进行预测，然后对各网络的预测进行求和，最后用argmax方法进行预测。</li>
<li>图2(d): DiCNN-b也包含两个独立学习特性的CNNs。然而，它是对提取的特征进行合并，然后另一个CNN将学习这些合并的特征并做出最后的预测。</li>
</ul>
</blockquote>
<h1 id="3、实验"><a href="#3、实验" class="headerlink" title="3、实验"></a>3、实验</h1><h2 id="3-1-数据集和实验设计"><a href="#3-1-数据集和实验设计" class="headerlink" title="3.1 数据集和实验设计"></a>3.1 数据集和实验设计</h2><p>本节对2013年中国安徽无人机采集的VHR图像数据集进行实验（图3）。数据集的空间分辨率为0.5m和大小是10595×9748像素。数据包含三个光谱波段(红、绿、蓝)。本文的目的是对图像中建筑、裸地、道路、绿地和水进行正确分类。</p>
<div align="center"> <img src="/2018/09/28/paper-SiCNN/data.png" alt="图3： 实验数据"> </div>

<p>在实验中，首先对原始图像进行了正确的预处理，然后使用基于图论的最小生成树分割算法<sup><a href="#footnote2">2</a></sup> <sup><a href="#footnote3">3</a></sup>进行影像分割。我们随机选取其中7093个作为训练样本，1520个作为验证样本，1521个作为测试样本。样本的分类分布如表1所示。我们采用总体精度(OA)、精度(precision, PA),召回率(recall, UA),κ系数(Kappa系数)和F-measure(F测度)作为精度评价指标。</p>
<p><img src="/2018/09/28/paper-SiCNN/samples.png" alt="表1：每个类中图像样本的数量"></p>
<h2 id="3-2-SiCNNs和DiCNNs分类结果"><a href="#3-2-SiCNNs和DiCNNs分类结果" class="headerlink" title="3.2 SiCNNs和DiCNNs分类结果"></a>3.2 SiCNNs和DiCNNs分类结果</h2><p>我们在数据集中训练了上文提出的SiCNN-3、SiCNN-4、DiCNN-a和DiCNN-b网络模型。每个类别和总体分类结果如表2所示。</p>
<p><img src="/2018/09/28/paper-SiCNN/results.png" alt="表2：本文所提出网络分类结果"></p>
<p>结果表明：在四种模型中，DiCNN-b具有最优的性能。此外，SiCNN-4和DiCNN-b都比SiCNN-3有更好的性能，说明增加分割对象外的信息(二值掩膜)可以提高分类结果。</p>
<h2 id="3-3-与传统OBIC方法比较"><a href="#3-3-与传统OBIC方法比较" class="headerlink" title="3.3 与传统OBIC方法比较"></a>3.3 与传统OBIC方法比较</h2><p>本小节对比了本文所提出的模型和传统的OBIC方法。传统的OBIC方法分为两类算法：一类将分割对象原始图像块直接作为输入；另一类是利用从分割对象提取出来的一维手工特征。具体结果如表3所示，样本标记结果如图4所示。</p>
<p><img src="/2018/09/28/paper-SiCNN/comparison.png" alt="表3：本文提出的模型和传统OBIC方法在安徽数据集上的分类结果"></p>
<p><img src="/2018/09/28/paper-SiCNN/image_result.png" alt="图6： 影像分类方法对比，第一行是基于分割对象原始图像特征的传统OBIC方法的结果；第二行是基于分割对象手工提取特征的传统OBIC方法的结果；第三行中的图像是DiCNN-b的结果"></p>
<h1 id="4、结论"><a href="#4、结论" class="headerlink" title="4、结论"></a>4、结论</h1><p>本文提出了一种新的基于监督深度CNN的OBIC框架来处理由图像分割算法分割的分割对象。然后介绍了框架中的两种掩码策略和四种网络模型。实验结果表明，与其他模型和传统的OBIC框架相比，DiCNN-b模型在大数据集中的性能更好，而SiCNN-3在小数据集中的性能更好。</p>
<p><strong>如何引用本文：</strong><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="section">@article&#123;doi:10.1080/2150704X.2017.1422873,</span></div><div class="line">author = &#123;Xiaodong Zhang and Qing Wang and Guanzhou Chen and Fan Dai and Kun Zhu and Yuanfu Gong and Yijuan Xie&#125;,</div><div class="line">title = &#123;An object-based supervised classification framework for very-high-resolution remote sensing images using convolutional neural networks&#125;,</div><div class="line">journal = &#123;Remote Sensing Letters&#125;,</div><div class="line">volume = &#123;9&#125;,</div><div class="line">number = &#123;4&#125;,</div><div class="line">pages = &#123;373-382&#125;,</div><div class="line">year  = &#123;2018&#125;,</div><div class="line">publisher = &#123;Taylor &amp; Francis&#125;,</div><div class="line">doi = &#123;10.1080/2150704X.2017.1422873&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<p><a name="footnote1">1</a> Zhao， W.， S. Du， and W. J. Emery. 2017. “Object-Based Convolutional Neural Network for HighResolution Imagery Classification.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing PP 99: 1–11.<br><a name="footnote2">2</a> Felzenszwalb, P. F., and D. P. Huttenlocher. 2004. “Efficient Graph-Based Image Segmentation.”International Journal of Computer Vision 59 (2): 167-181.doi:10.1023/B:VISI.0000022288.19776.77.<br><a name="footnote3">3</a> Cui, W. 2010. “Research on Graph Theory Based Object Oriented High Resolution ImageSegmentation.” PhD diss., Wuhan University.</p>

  </article>
  <div class="random-toc-area">
  <button class="btn-hide-toc btn-hide-toc-show" style="display: none" onclick="TOCToggle()">显示目录</button>
  <button class="btn-hide-toc btn-hide-toc-hide" onclick="TOCToggle()">隐藏目录</button>
  <div class="random-toc">
    <h2>目录</h2>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1、简介"><span class="toc-text">1、简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2、方法"><span class="toc-text">2、方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-分割对象预处理"><span class="toc-text">2.1 分割对象预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-影像分类网络结构"><span class="toc-text">2.2 影像分类网络结构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3、实验"><span class="toc-text">3、实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-数据集和实验设计"><span class="toc-text">3.1 数据集和实验设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-SiCNNs和DiCNNs分类结果"><span class="toc-text">3.2 SiCNNs和DiCNNs分类结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-与传统OBIC方法比较"><span class="toc-text">3.3 与传统OBIC方法比较</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4、结论"><span class="toc-text">4、结论</span></a></li></ol>
  </div>
</div>

  
<nav id="pagination">
  
    <a href="/2018/09/27/paper-sdfcn/" class="prev">&larr; 上一篇 基于深度对称密集连接全卷积网络的高分辨率遥感影像语义分割框架 SDFCN</a>
  

  

  
    <a href="/2018/09/29/paper-kdcnn/" class="next">下一篇 基于知识蒸馏的小规模遥感影像场景分类网络模型训练方法 &rarr;</a>
  
</nav>

  <!-- JiaThis Button BEGIN -->

<!-- JiaThis Button END -->


      
      
      
    </div>
  </div>

  <div id="bottom-outer">
    <div id="bottom-inner">
      Site by HELOBILLY using
      <a href="http://hexo.io">Hexo</a> & <a href="https://github.com/stiekel/hexo-theme-random">Random</a>
      <br>
      
    </div>
  </div>
</div>

</div>



<div id="user-card">
  <div class="center-field">
    <img class="avatar" src="/helobilly.jpg">
    <p id="description"></p>
    <ul class="social-icon">
  
  
    <li>
      <a href="https://github.com/HELOBILLY">
        
          <i class="icon iconfont github">&#xe606;</i>
        
      </a>
    </li>
  
    <li>
      <a href="https://coding.net/u/HELOBILLY">
        
          <i class="icon iconfont coding">&#xe607;</i>
        
      </a>
    </li>
  
    <li>
      <a href="https://cveo.github.io/">
        
          CVEO
        
      </a>
    </li>
  
</ul>
  </div>
</div>


<div id="btn-view">Hide</div>

<script>
// is trigger analytics / tongji script
var isIgnoreHost = false;

if(window && window.location && window.location.host) {
  isIgnoreHost = ["localhost","127.0.0.1"].some(function(address){
    return 0 === window.location.host.indexOf(address);
  });
}

var isTriggerAnalytics = !( true && isIgnoreHost );

</script>




  
  
    <script src="/js/jquery-2.2.3.min.js"></script>
  
    <script src="/js/vegas.min.js"></script>
  
    <script src="/js/random.js"></script>
  
    <script src="/js/highlight.pack.js"></script>
  
    <script src="/js/jquery.mousewheel.pack.js"></script>
  
    <script src="/js/jquery.fancybox.pack.js"></script>
  
    <script src="/js/jquery.fancybox-thumbs.js"></script>
  
    <script src="/js/plyr.js"></script>
  

<script>

  // fancybox
  var backgroundImages = [];
  
  $('#post').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox') || $(this).parent().hasClass('fancybox-thumb')) return;
      var alt = this.alt || this.title;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'post' + i);
    });
  });
  $(".fancybox").fancybox();

var vegasConfig = {"preload­Image":true,"transition":["slideLeft2","slideRight2","flash2"],"timer":true,"delay":5000,"shuffle":true,"count":28};
var unsplashConfig = {"gravity":"north"};
// is show background images
var turnoffBackgroundImage = false;



  turnoffBackgroundImage = true;


var backgroundColor = "34495E";

$(".fancybox-thumb").fancybox({
  prevEffect: 'none',
  nextEffect: 'none',
  helpers: {
    title: {
      type: 'outside'
    },
    thumbs: {
      width: 50,
      height: 50
    }
  }
});

// show video with plyr
$(".video-container iframe").each(function(i){
  var url = $(this).attr('src');
  var id = url.split('/').pop();
  var plyrContainer = document.createElement('div');
  plyrContainer.className = 'plyr';
  var plyrElement = document.createElement('div');
  plyrElement.dataset.videoId = id;
  switch(true) {
    case url.search('youtube.com') >= 0:
      plyrElement.dataset.type = 'youtube';
      break;
    case url.search('vimeo.com') >= 0:
      plyrElement.dataset.type = 'vimeo';
      break;
    default:
      return;
  };
  plyrContainer.appendChild(plyrElement);
  $(this).parent().html(plyrContainer);
});
plyr.setup('.plyr', {iconUrl: '/css/sprite.svg'});
</script>
</body>
</html>

